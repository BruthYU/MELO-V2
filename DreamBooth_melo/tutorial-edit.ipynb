{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 1: Import packages and hydra configurations\n",
    "- Please use `Python=3.8+` with `Pytorch==1.13.1` and `diffusers==0.23.0`\n",
    "- Configurations are governed by [Hydra configs](https://hydra.cc/docs/configure_hydra/intro/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alg: melo_diff\n",
      "train_text_encoder: true\n",
      "learning_rate: 1.0e-05\n",
      "validation_prompt: a photo of {} in the Acropolis\n",
      "task: diffusion\n",
      "pretrained_model_name_or_path: CompVis/stable-diffusion-v1-4\n",
      "pretrained_cache_dir: pretrained_stable_diffusion\n",
      "revision: null\n",
      "tokenizer_name: null\n",
      "class_data_dir: class_data\n",
      "instance_prompt: a photo of\n",
      "class_prompt: a photo of\n",
      "with_prior_preservation: false\n",
      "prior_loss_weight: 0.1\n",
      "num_class_images: 40\n",
      "seed: null\n",
      "resolution: 512\n",
      "center_crop: false\n",
      "train_batch_size: 1\n",
      "sample_batch_size: 4\n",
      "num_train_epochs: 1\n",
      "max_train_steps: 800\n",
      "checkpoints_total_limit: null\n",
      "resume_from_checkpoint: null\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: false\n",
      "scale_lr: false\n",
      "lr_scheduler: constant\n",
      "lr_warmup_steps: 0\n",
      "lr_num_cycles: 1\n",
      "lr_power: 1.0\n",
      "use_8bit_adam: false\n",
      "dataloader_num_workers: 0\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_weight_decay: 0.01\n",
      "adam_epsilon: 1.0e-08\n",
      "max_grad_norm: 1.0\n",
      "push_to_hub: false\n",
      "hub_token: null\n",
      "hub_model_id: null\n",
      "allow_tf32: true\n",
      "num_validation_images: 4\n",
      "validation_steps: 1200\n",
      "mixed_precision: null\n",
      "prior_generation_precision: null\n",
      "local_rank: -1\n",
      "enable_xformers_memory_efficient_attention: false\n",
      "set_grads_to_none: false\n",
      "offset_noise: false\n",
      "snr_gamma: null\n",
      "pre_compute_text_embeddings: false\n",
      "tokenizer_max_length: null\n",
      "text_encoder_use_attention_mask: false\n",
      "skip_save_text_encoder: false\n",
      "validation_images: null\n",
      "class_labels_conditioning: false\n",
      "validation_scheduler: DPMSolverMultistepScheduler\n",
      "name: null\n",
      "output_dir: text-inversion-model\n",
      "num_dataloader_workers: 1\n",
      "no_trace_malloc: false\n",
      "check_dir: null\n",
      "checkpoint_step: 400\n",
      "locality_seed: 5\n",
      "model:\n",
      "  fan_in_fan_out: false\n",
      "  use_lora: true\n",
      "  num_block: 30\n",
      "  num_edit_per_block: 1\n",
      "  num_rank_per_block:\n",
      "    text_encoder: 16\n",
      "    unet: 16\n",
      "  lora_dropout: 0.0\n",
      "  lora_bias: none\n",
      "  lora_text_encoder_dropout: 0.0\n",
      "  lora_text_encoder_bias: none\n",
      "  UNET_TARGET_MODULES:\n",
      "  - to_q\n",
      "  - to_v\n",
      "  - query\n",
      "  - value\n",
      "  TEXT_ENCODER_TARGET_MODULES:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "version: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/anaconda3/envs/melo_v2/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from hydra import compose, initialize\n",
    "import os\n",
    "import transformers\n",
    "import warnings\n",
    "import json\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from DreamBooth_melo.trainer.melo_trainer import *\n",
    "from DreamBooth_melo.database.router import *\n",
    "os.environ['http_proxy'] = '127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = '127.0.0.1:7890'\n",
    "import numpy as np\n",
    "# Load Configurations governed by hydra\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config = compose(config_name=\"config\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### According to [DreamBooth](https://arxiv.org/pdf/2208.12242), prior preservation are needed to prevent language drift. While Multi-MELO could dynamically activate LoRA blocks, knowledge are retained when no specific blocks are activated, so we set `config.with_prior_preservation` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_config(config):\n",
    "    base_dir = '.'\n",
    "    config.class_data_dir = os.path.join(base_dir, config.class_data_dir)\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != config.local_rank:\n",
    "        config.local_rank = env_local_rank\n",
    "    if config.with_prior_preservation:\n",
    "        if config.class_data_dir is None:\n",
    "            raise ValueError(\"You must specify a data directory for class images.\")\n",
    "        if config.class_prompt is None:\n",
    "            raise ValueError(\"You must specify prompt for class images.\")\n",
    "    else:\n",
    "        if config.class_data_dir is not None:\n",
    "            warnings.warn(\"You need not use --class_data_dir without --with_prior_preservation.\")\n",
    "        if config.class_prompt is not None:\n",
    "            warnings.warn(\"You need not use --class_prompt without --with_prior_preservation.\")\n",
    "    if config.train_text_encoder and config.pre_compute_text_embeddings:\n",
    "        raise ValueError(\"`--train_text_encoder` cannot be used with `--pre_compute_text_embeddings`\")\n",
    "\n",
    "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n",
    "\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    elif model_class == \"T5EncoderModel\":\n",
    "        from transformers import T5EncoderModel\n",
    "\n",
    "        return T5EncoderModel\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 2: Check import configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-DIFF CONFIG-]  class_prompt: a photo of\n",
      "[-DIFF CONFIG-]  with_prior_preservation: False\n",
      "[-DIFF CONFIG-]  prior_loss_weight: 0.1\n",
      "[-DIFF CONFIG-]  learning_rate: 1e-05\n",
      "[-MELO CONFIG-]  UNET_TARGET_MODULES: ['to_q', 'to_v', 'query', 'value']\n",
      "[-MELO CONFIG-]  TEXT_ENCODER_TARGET_MODULES: ['q_proj', 'v_proj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40119/1450944420.py:14: UserWarning: You need not use --class_data_dir without --with_prior_preservation.\n",
      "  warnings.warn(\"You need not use --class_data_dir without --with_prior_preservation.\")\n",
      "/tmp/ipykernel_40119/1450944420.py:16: UserWarning: You need not use --class_prompt without --with_prior_preservation.\n",
      "  warnings.warn(\"You need not use --class_prompt without --with_prior_preservation.\")\n"
     ]
    }
   ],
   "source": [
    "LOG.info(\"*MELO* Dreambooth\")\n",
    "check_config(config)\n",
    "\n",
    "diff_config_keys = ['class_prompt', 'with_prior_preservation', 'prior_loss_weight', 'learning_rate']\n",
    "melo_config_keys = ['UNET_TARGET_MODULES', 'TEXT_ENCODER_TARGET_MODULES']\n",
    "DIFF_CONFIG = dict(config)\n",
    "MELO_CONFIG = dict(config.model)\n",
    "for k in diff_config_keys:\n",
    "    print(f'[-DIFF CONFIG-]  {k}: {DIFF_CONFIG[k]}')\n",
    "for k in melo_config_keys:\n",
    "    print(f'[-MELO CONFIG-]  {k}: {MELO_CONFIG[k]}')\n",
    "\n",
    "base_dir = '.'\n",
    "with open_dict(config):\n",
    "    config.base_dir = base_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Edit 3 personal objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "/home/yu/anaconda3/envs/melo_v2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'timestep_spacing', 'prediction_type', 'sample_max_value', 'variance_type', 'thresholding', 'clip_sample_range', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "{'time_cond_proj_dim', 'resnet_skip_time_act', 'encoder_hid_dim_type', 'class_embeddings_concat', 'dropout', 'resnet_time_scale_shift', 'dual_cross_attention', 'mid_block_type', 'projection_class_embeddings_input_dim', 'addition_embed_type', 'num_attention_heads', 'addition_embed_type_num_heads', 'mid_block_only_cross_attention', 'num_class_embeds', 'time_embedding_dim', 'reverse_transformer_layers_per_block', 'conv_out_kernel', 'addition_time_embed_dim', 'cross_attention_norm', 'upcast_attention', 'time_embedding_type', 'time_embedding_act_fn', 'use_linear_projection', 'conv_in_kernel', 'resnet_out_scale_factor', 'attention_type', 'only_cross_attention', 'transformer_layers_per_block', 'timestep_post_act', 'encoder_hid_dim', 'class_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "Steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [03:23<00:00,  3.93it/s, loss=0.26, lr=1e-5]\n",
      "Steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [03:34<00:00,  3.73it/s, loss=0.00631, lr=1e-5]\n",
      "Steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [03:53<00:00,  3.42it/s, loss=0.00726, lr=1e-5]\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision\n",
    ")\n",
    "\n",
    "# Gradient Accumulation is not supported for multi_gpu setting\n",
    "if config.train_text_encoder and config.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n",
    "    raise ValueError(\n",
    "        \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n",
    "        \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n",
    "    )\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if config.seed is not None:\n",
    "    set_seed(config.seed)\n",
    "\n",
    "'''\n",
    "Load Model\n",
    "'''\n",
    "text_encoder_cls = import_model_class_from_model_name_or_path(config.pretrained_model_name_or_path,\n",
    "                                                              config.revision)\n",
    "# Load scheduler and models\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(config.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "text_encoder = text_encoder_cls.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=config.revision)\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"vae\", revision=config.revision)\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config.pretrained_model_name_or_path, subfolder=\"unet\", revision=config.revision)\n",
    "\n",
    "'''\n",
    "Load tokenizer\n",
    "'''\n",
    "if config.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name, revision=config.revision, use_fast=False)\n",
    "elif config.pretrained_model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=config.revision,\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "'''\n",
    "Algorithm Initialization\n",
    "'''\n",
    "alg_module = importlib.import_module(f'algs.{config.alg}')\n",
    "AlgClass = getattr(alg_module, config.alg.upper())\n",
    "alg = AlgClass(accelerator, tokenizer, noise_scheduler, vae, unet, text_encoder, config)\n",
    "\n",
    "'''\n",
    "data_info\n",
    "'''\n",
    "with open(os.path.join(base_dir, \"data\",\"data.json\"), 'r') as f:\n",
    "    data_info = json.load(f)\n",
    "#subject_list = data_info.keys()\n",
    "subject_list = list(data_info.keys())[:3]\n",
    "identifier_list = np.load(os.path.join(base_dir, \"data/rare_tokens/rare_tokens.npy\"))[:len(subject_list)]\n",
    "\n",
    "'''\n",
    "Trainer\n",
    "'''\n",
    "trainer = dream_trainer(config, alg, accelerator, tokenizer, None, data_info, subject_list, identifier_list)\n",
    "torch.cuda.empty_cache()\n",
    "trainer.run_edit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
